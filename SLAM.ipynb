{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1HTsdFD2jC9SyWnZDUAIkGxF3RA-Qx0ch","timestamp":1686207293444},{"file_id":"17C1jonBsqiej8G8XIQtpzGtqTZVenwTl","timestamp":1686108151670}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This is the pipeline of MonoSLAM (Real-Time Single Camera SLAM). The concepts and techniques used here are not only fundamental to SLAM but also applicable to other domains, such as multi-view geometry. Let's get started and dive into the exciting world of SLAM!"],"metadata":{"id":"qrAEX_YBN3hq"}},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","\n","from google.colab.patches import cv2_imshow\n","from IPython.display import clear_output"],"metadata":{"id":"4_yyTDzaS42Q","executionInfo":{"status":"ok","timestamp":1686260863863,"user_tz":420,"elapsed":760,"user":{"displayName":"Evan Zhao","userId":"02301079516512422554"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["Let's first define some useful classes - we will need to keep track of intrinsic and extrinsic camera parameters and every frame:"],"metadata":{"id":"-F9OFfc_dYXM"}},{"cell_type":"code","source":["!curl -O https://gitlab.cs.washington.edu/yueqianz/439v-hw7-slam/-/blob/main/road.mp4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5KTP7MUv6prv","executionInfo":{"status":"ok","timestamp":1686260868467,"user_tz":420,"elapsed":1931,"user":{"displayName":"Evan Zhao","userId":"02301079516512422554"}},"outputId":"0f9e4e5b-62d3-4a0b-821d-9787bb3a73df"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 84037    0 84037    0     0  47775      0 --:--:--  0:00:01 --:--:-- 47775\n"]}]},{"cell_type":"code","source":["class Frame(object):\n","    # global\n","    idx = 0\n","    last_kps, last_des, last_pose = None, None, None\n","\n","    # for indivisual frame, keep track of info from last frame\n","    def __init__(self, image):\n","        Frame.idx += 1\n","\n","        self.image = image\n","        self.idx   = Frame.idx\n","        self.last_kps  = Frame.last_kps\n","        self.last_des  = Frame.last_des\n","        self.last_pose = Frame.last_pose\n","\n","class Camera(object):\n","    W, H, F = 960, 540, 270\n","    P = np.eye(3)"],"metadata":{"id":"e0buMdb_bPHz","executionInfo":{"status":"ok","timestamp":1686260868468,"user_tz":420,"elapsed":2,"user":{"displayName":"Evan Zhao","userId":"02301079516512422554"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["First, we want to extract keypoints from each frame. ORB (Oriented FAST and Rotated BRIEF) features consist of two components: keypoints and descriptors. The keypoints, known as \"Oriented FAST,\" are an improved version of FAST corners, while the descriptors are referred to as BRIEF."],"metadata":{"id":"AAA0SGOMhJ--"}},{"cell_type":"code","source":["# TODO:\n","def extract_points(frame):\n","    orb = cv2.ORB_create()\n","\n","    # Convert frame.image to grayscale\n","    image = cv2.cvtColor(frame.image, cv2.COLOR_BGR2GRAY)\n","\n","    # Detect corners using cv2.goodFeaturesToTrack()\n","    pts = cv2.goodFeaturesToTrack(image, 3000, qualityLevel=0.01, minDistance=3)\n","\n","    # Extract features using cv2.KeyPoint()\n","    kps = [cv2.KeyPoint(x=pt[0][0], y=pt[0][1], size=20) for pt in pts]\n","\n","    # Compute the features of the corners using the compute function from the ORB module\n","    # This yields two outputs: kps and des\n","    kps, des = orb.compute(image, kps)\n","\n","    # Convert kps to numpy array\n","    kps = np.array([(kp.pt[0], kp.pt[1]) for kp in kps])\n","\n","    return kps, des"],"metadata":{"id":"i_H-kMEDhCZo","executionInfo":{"status":"ok","timestamp":1686260870188,"user_tz":420,"elapsed":2,"user":{"displayName":"Evan Zhao","userId":"02301079516512422554"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["Then, we need match the extracted keypoints between consecutive frames. Here, the match_points function will align the keypoints of the current frame with those of the previous frame. This step enables us to understand the camera movements in the 3D scene and thus construct the environment later."],"metadata":{"id":"PctOR7N0e2uN"}},{"cell_type":"code","source":["# TODO: \n","def match_points(frame):\n","    # Create a matcher called bfmatch using cv2.BFMatcher\n","    # Set normType to cv2.NORM_HAMMING for ORB descriptors\n","    # Set crossCheck to False (default value)\n","    bfmatch = cv2.BFMatcher(cv2.NORM_HAMMING)\n","\n","    # Retrieve the top 2 best matches for each keypoint\n","    matches = bfmatch.knnMatch(frame.curr_des, frame.last_des, k=2)\n","\n","    match_kps = []\n","    idx1, idx2 = [], []\n","\n","    # Iterate over the matches and filter based on distance ratio\n","    for m, n in matches:\n","        if m.distance < 0.75 * n.distance:\n","            idx1.append(m.queryIdx)\n","            idx2.append(m.trainIdx)\n","\n","            p1 = frame.curr_kps[m.queryIdx]\n","            p2 = frame.last_kps[m.trainIdx]\n","            match_kps.append((p1, p2))\n","\n","    # Update keypoints to include only the matched keypoints\n","    frame.curr_kps = frame.curr_kps[idx1]\n","    frame.last_kps = frame.last_kps[idx2]\n","\n","    return match_kps"],"metadata":{"id":"79_Ymk7kt560","executionInfo":{"status":"ok","timestamp":1686260872216,"user_tz":420,"elapsed":2,"user":{"displayName":"Evan Zhao","userId":"02301079516512422554"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["After implementing the extract-match, you can visualize the movements of keypoints now! upload some videos your like on colab via the \"Files\" icon in the left colomn -> upload to session storage. we have provided you with some test videos [here](https://gitlab.cs.washington.edu/yueqianz/439v-hw7-slam). Then, run the code below."],"metadata":{"id":"wUzWnFYF07Na"}},{"cell_type":"code","execution_count":15,"metadata":{"id":"eDTmbug8sp4n","executionInfo":{"status":"ok","timestamp":1686260874236,"user_tz":420,"elapsed":2,"user":{"displayName":"Evan Zhao","userId":"02301079516512422554"}}},"outputs":[],"source":["# helper function to plot the motion trajectory of keypoints, you can follow these steps:\n","def draw_points(frame):\n","    for kp1, kp2 in zip(frame.curr_kps, frame.last_kps):\n","        u1, v1 = int(kp1[0]), int(kp1[1])\n","        u2, v2 = int(kp2[0]), int(kp2[1])\n","        cv2.circle(frame.image, (u1, v1), color=(0,0,255), radius=3)\n","        cv2.line(frame.image, (u1, v1), (u2, v2), color=(255,0,0))\n","    return None"]},{"cell_type":"code","source":["cap = cv2.VideoCapture(\"road.mp4\")  # TODO: change the file name here to match your uploaded file\n","while cap.isOpened() :\n","    ret, image = cap.read()\n","    frame = Frame(image)\n","\n","    if ret:\n","        frame.curr_kps, frame.curr_des = extract_points(frame)\n","        Frame.last_kps, Frame.last_des = frame.curr_kps, frame.curr_des\n","        if frame.idx > 1:\n","          match_kps = match_points(frame)\n","          draw_points(frame)\n","    else:\n","        break\n","    clear_output(wait=True)\n","    cv2_imshow(frame.image)\n","    k = cv2.waitKey(33) & 0xFF\n","    if k == 27:\n","      cv2.destroyAllWindows()"],"metadata":{"id":"l__o0Ynq06ak","executionInfo":{"status":"ok","timestamp":1686260875606,"user_tz":420,"elapsed":2,"user":{"displayName":"Evan Zhao","userId":"02301079516512422554"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["Although the current approach is satisfactory, there is room for improvement. Notice that there are some ourliers in the video. We can enhance the estimation process by leveraging the RANSAC (RAndom SAmple Consensus) algorithm, which enable us to mitigate the influence of noise and outliers"],"metadata":{"id":"DsB4kagu6fCx"}},{"cell_type":"markdown","source":["RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set. Each iteration performs the following tasks:\n","\n","1. Select min_samples random samples from the original data and check whether the set of data is valid (see is_data_valid).\n","\n","2. Estimate a model to the random subset (model_cls.estimate(*data[random_subset]) and check whether the estimated model is valid (see is_model_valid).\n","\n","3. Classify all data as inliers or outliers by calculating the residuals to the estimated model (model_cls.residuals(*data)) - all data samples with residuals smaller than the residual_threshold are considered as inliers.\n","\n","4. Save estimated model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has less sum of residuals.\n","\n","These steps are performed either a maximum number of times or until one of the special stop criteria are met. The final model is estimated using all inlier samples of the previously determined best model."],"metadata":{"id":"y9M2POAmhEAH"}},{"cell_type":"markdown","source":["To make your life easier, we are going to use the consistency check (model.estimate) in scikit-image for keypoints. We prefer essential matrix over the homography when dealing with general camera motion because it can model non-planar motion, allowing for accurate pose estimation and 3D reconstruction. we have provide a skeleton for you, and your job is to fill the iterative algorithm described above in ransac()."],"metadata":{"id":"S8GckieM8U3f"}},{"cell_type":"code","source":["from skimage.transform import EssentialMatrixTransform"],"metadata":{"id":"xBuYL8x7Slot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# normalize the coordinates of keypoints using camera intrinsic parameters\n","def normalize_coordinates(K, keypoints):\n","    K_inv = np.linalg.inv(K)\n","    add_ones = lambda x: np.concatenate([x, np.ones((x.shape[0], 1))], axis=1)\n","    normalized_pts = np.dot(K_inv, add_ones(keypoints).T).T[:, :2]\n","    return normalized_pts\n","\n","def noice_reduce(match_kps):\n","    K = np.array([[Camera.F, 0, Camera.W//2],\n","                  [0, Camera.F, Camera.H//2],\n","                  [0, 0, 1]])\n","    matched_keypoints = np.array(matched_keypoints)\n","    normalized_curr_kps = normalize_coordinates(K, matched_keypoints[:, 0])\n","    normalized_last_kps = normalize_coordinates(K, matched_keypoints[:, 1])\n","\n","    # Find the inlier data and their corresponding essential matrix.\n","    model, inliers = ransac((normalized_last_kps, normalized_curr_kps),\n","                            EssentialMatrixTransform,\n","                            min_samples=8,\n","                            residual_threshold=0.005,\n","                            max_trials=200)\n","\n","    frame.curr_kps = frame.curr_kps[inliers]\n","    frame.last_kps = frame.last_kps[inliers]\n","\n","\n","# TODO: implement the iterative part\n","def ransac(data, model_class, min_samples, residual_threshold, max_trials):\n","    best_inlier_num = 0\n","    best_inlier_residuals_sum = np.inf\n","    best_inliers = []\n","    num_samples = len(data[0]) # total number of samples\n","    # estimate model for current random sample set\n","    model = model_class()\n","    num_trials = 0\n","    rng = np.random.default_rng()\n","    \n","    # max_trials can be updated inside the loop, so this cannot be a for-loop\n","    while num_trials < max_trials:\n","        num_trials += 1\n","\n","        spl_idxs = rng.choice(num_samples, min_samples, replace=False)\n","        # do sample selection according data pairs\n","        samples = [d[spl_idxs] for d in data]\n","\n","        success = model.estimate(*samples)\n","        # backwards compatibility\n","        if success is not None and not success:\n","            continue\n","\n","        residuals = np.abs(model.residuals(*data))\n","        # consensus set / inliers\n","        inliers = residuals < residual_threshold\n","        residuals_sum = residuals.dot(residuals)\n","\n","        # choose as new best model if number of inliers is maximal\n","        inliers_count = np.count_nonzero(inliers)\n","        if (\n","            # more inliers\n","            inliers_count > best_inlier_num\n","            # same number of inliers but less \"error\" in terms of residuals\n","            or (inliers_count == best_inlier_num\n","                and residuals_sum < best_inlier_residuals_sum)):\n","            best_inlier_num = inliers_count\n","            best_inlier_residuals_sum = residuals_sum\n","            best_inliers = inliers\n","\n","    # estimate final model using all inliers\n","    if any(best_inliers):\n","        # select inliers for each data array\n","        data_inliers = [d[best_inliers] for d in data]\n","        model.estimate(*data_inliers)\n","    else:\n","        model = None\n","        best_inliers = None\n","\n","    return model, best_inliers"],"metadata":{"id":"4WVELC1e9_Uq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, if you run the code below, you should observe a notable reduction in noise!"],"metadata":{"id":"Qiw-XxW8Hg3N"}},{"cell_type":"code","source":["cap = cv2.VideoCapture(\"road.mp4\")  # TODO: change the file name here to match your uploaded file\n","while cap.isOpened() :\n","    ret, image = cap.read()\n","    frame = Frame(image)\n","\n","    if ret:\n","        frame.curr_kps, frame.curr_des = extract_points(frame)\n","        Frame.last_kps, Frame.last_des = frame.curr_kps, frame.curr_des\n","        if frame.idx > 1:\n","          match_kps = match_points(frame)\n","          noice_reduce(match_kps)\n","          draw_points(frame)\n","    else:\n","        break\n","    clear_output(wait=True)\n","    cv2_imshow(frame.image)\n","    k = cv2.waitKey(33) & 0xFF\n","    if k == 27:\n","      cv2.destroyAllWindows()"],"metadata":{"id":"LHuecgTUHgmk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Up to this point, you have gain a solid understanding of how to match points in a 2D space. Now, let's move to the conceptual part. You will practice how to transform these 2D points into 3D using the best model obtained from ransac(). This will enable you later to construct the complete 3D scene."],"metadata":{"id":"7gFM8DlgTkHF"}},{"cell_type":"code","source":[],"metadata":{"id":"mddoN01DzRqM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# New Section"],"metadata":{"id":"U5T3yDaNzUEz"}}]}